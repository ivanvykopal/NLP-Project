{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/my-conda-envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, strip_tags, strip_multiple_whitespaces, strip_numeric, strip_short, strip_short, strip_numeric, strip_punctuation, strip_tags, strip_multiple_whitespaces, remove_stopwords\n",
    "from nltk.stem import SnowballStemmer, PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from stemmers.stemmsk import stem as stem_sk\n",
    "from stemmers.stemmcz import stem_word as stem_cz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(language):\n",
    "    stopwords = []\n",
    "    with open('./stopwords/{}.txt'.format(language), 'r') as f:\n",
    "        for line in f:\n",
    "            stopwords.append(line.strip())\n",
    "    return stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(text, language):\n",
    "    stopwords = load_stopwords(language)\n",
    "    if language == 'slovak':\n",
    "        stem = stem_sk\n",
    "    elif language == 'czech':\n",
    "        stem = stem_cz\n",
    "    elif language == 'english':\n",
    "        stemmer = PorterStemmer()\n",
    "        stem = stemmer.stem\n",
    "    elif language == 'german':\n",
    "        stemmer = SnowballStemmer(\"german\")\n",
    "        stem = stemmer.stem\n",
    "\n",
    "    data = text.lower()\n",
    "    data = strip_tags(data)\n",
    "    data = strip_punctuation(data)\n",
    "    data = strip_multiple_whitespaces(data)\n",
    "    data = strip_numeric(data)\n",
    "    data = remove_stopwords(data, stopwords=stopwords)\n",
    "    data = strip_short(data, minsize=3)\n",
    "    data = stem(data)\n",
    "\n",
    "    return data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the overall number of tokens for each lanugage\n",
    "data = load_dataset(\"wikipedia\", language='sk', date=\"20231101\", beam_runner='DirectRunner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 242235/242235 [04:57<00:00, 815.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# count the overall number of tokens from data['train]\n",
    "count = 0\n",
    "for i in tqdm(range(len(data['train']))):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    count += len(preprocess_string(data['train'][i]['text'], 'slovak'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "slovak_count = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikipedia\", \"20220301.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▌                                                                                                                                     | 29149/6458670 [10:52<39:59:00, 44.67it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jovyan/workspace/NLP/data/Data analysis.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bjupyter-ivanvykopal-/home/jovyan/workspace/NLP/data/Data%20analysis.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m english_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bjupyter-ivanvykopal-/home/jovyan/workspace/NLP/data/Data%20analysis.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(dataset[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]))):\n\u001b[0;32m----> <a href='vscode-notebook-cell://tunnel%2Bjupyter-ivanvykopal-/home/jovyan/workspace/NLP/data/Data%20analysis.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     english_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(preprocess_string(dataset[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m][i][\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39m'\u001b[39;49m\u001b[39menglish\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "\u001b[1;32m/home/jovyan/workspace/NLP/data/Data analysis.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bjupyter-ivanvykopal-/home/jovyan/workspace/NLP/data/Data%20analysis.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m data \u001b[39m=\u001b[39m strip_multiple_whitespaces(data)\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bjupyter-ivanvykopal-/home/jovyan/workspace/NLP/data/Data%20analysis.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m data \u001b[39m=\u001b[39m strip_numeric(data)\n\u001b[0;32m---> <a href='vscode-notebook-cell://tunnel%2Bjupyter-ivanvykopal-/home/jovyan/workspace/NLP/data/Data%20analysis.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m data \u001b[39m=\u001b[39m remove_stopwords(data, stopwords\u001b[39m=\u001b[39;49mstopwords)\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bjupyter-ivanvykopal-/home/jovyan/workspace/NLP/data/Data%20analysis.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m data \u001b[39m=\u001b[39m strip_short(data, minsize\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bjupyter-ivanvykopal-/home/jovyan/workspace/NLP/data/Data%20analysis.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m data \u001b[39m=\u001b[39m stem(data)\n",
      "File \u001b[0;32m~/my-conda-envs/nlp/lib/python3.10/site-packages/gensim/parsing/preprocessing.py:96\u001b[0m, in \u001b[0;36mremove_stopwords\u001b[0;34m(s, stopwords)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Remove :const:`~gensim.parsing.preprocessing.STOPWORDS` from `s`.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \n\u001b[1;32m     74\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m \n\u001b[1;32m     94\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m s \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mto_unicode(s)\n\u001b[0;32m---> 96\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(remove_stopword_tokens(s\u001b[39m.\u001b[39;49msplit(), stopwords))\n",
      "File \u001b[0;32m~/my-conda-envs/nlp/lib/python3.10/site-packages/gensim/parsing/preprocessing.py:118\u001b[0m, in \u001b[0;36mremove_stopword_tokens\u001b[0;34m(tokens, stopwords)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mif\u001b[39;00m stopwords \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     stopwords \u001b[39m=\u001b[39m STOPWORDS\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m [token \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stopwords]\n",
      "File \u001b[0;32m~/my-conda-envs/nlp/lib/python3.10/site-packages/gensim/parsing/preprocessing.py:118\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mif\u001b[39;00m stopwords \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     stopwords \u001b[39m=\u001b[39m STOPWORDS\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m [token \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stopwords]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "english_count = 0\n",
    "for i in tqdm(range(len(dataset['train']))):\n",
    "    english_count += len(preprocess_string(dataset['train'][i]['text'], 'english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42524655"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6458670/6458670 [1:31:55<00:00, 1170.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def preprocess_and_count(text):\n",
    "    preprocessed_text = preprocess_string(text, 'english')\n",
    "    return len(preprocessed_text)\n",
    "\n",
    "batch_size = 1000\n",
    "english_count2 = 0\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    futures = [executor.submit(preprocess_and_count, item['text']) for item in dataset['train']]\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        english_count2 += future.result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1555155883"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_count2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
